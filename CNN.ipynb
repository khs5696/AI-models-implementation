{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQdrTERmjQnQBaoAlawwm1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khs5696/AI504/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting"
      ],
      "metadata": {
        "id": "hZOOc9Az50aV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MOGGHXUQ5Pvv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter"
      ],
      "metadata": {
        "id": "IMWlWdk26H4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "learning_rate = 0.0002\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "H4WDnqaN6KZS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "Rheb8gJq6B8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = datasets.MNIST('./', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test = datasets.MNIST('./', train=False, transform=transforms.ToTensor(), download=True)\n",
        "train, valid = torch.utils.data.random_split(train, [50000, 10000])\n",
        "\n",
        "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "FDR32AY35RDp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168f2ada-8d0b-4aeb-a852-5d9c9ba9b0c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 45305337.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 117379548.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 23286456.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 20288103.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms_train = transforms.Compose([\n",
        "  transforms.RandomCrop(32, padding=4),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "transforms_test = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "cifar_train = datasets.CIFAR10(root='./', train=True, download=True, transform=transforms_train)\n",
        "cifar_test = datasets.CIFAR10(root='./', train=False, download=True, transform=transforms_test)\n",
        "cifar_loader = {}\n",
        "cifar_loader['train'] = DataLoader(cifar_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "cifar_loader['test'] = DataLoader(cifar_test, batch_size=batch_size, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YG6rMOr7fLd",
        "outputId": "10be37e3-fc65-4441-d1b2-b30482828f14"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 29134076.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to ./\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "JfeU4WYE6tJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=1, out_channels=32, kernel_size=7, stride=1, padding=0),\n",
        "      nn.BatchNorm2d(32),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=32, out_channels=64, kernel_size=7, stride=1, padding=0),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.fc = nn.Linear(64 * 16 * 16, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    output = self.fc(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "nTAfFIKI6qRA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "HHJhIbfJ6q7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "xoXeWLOQ-1RN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  epoch_loss = 0\n",
        "  print(\"-\"*10)\n",
        "  print(\"Epochs {}/{}\".format(epoch+1, epochs))\n",
        "\n",
        "  for data, label in train_dataloader:\n",
        "    data, label = data.to(device), label.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, label)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  epoch_loss += loss\n",
        "  print(\"Train_loss : {}\".format(epoch_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wZDS3d0_Lgx",
        "outputId": "a895c54b-3c90-4ffd-8308-2bbede1113f9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n",
            "Epochs 1/20\n",
            "Train_loss : 0.18714705109596252\n",
            "----------\n",
            "Epochs 2/20\n",
            "Train_loss : 0.21925446391105652\n",
            "----------\n",
            "Epochs 3/20\n",
            "Train_loss : 0.32460319995880127\n",
            "----------\n",
            "Epochs 4/20\n",
            "Train_loss : 0.20670290291309357\n",
            "----------\n",
            "Epochs 5/20\n",
            "Train_loss : 0.17359289526939392\n",
            "----------\n",
            "Epochs 6/20\n",
            "Train_loss : 0.14020200073719025\n",
            "----------\n",
            "Epochs 7/20\n",
            "Train_loss : 0.1866656243801117\n",
            "----------\n",
            "Epochs 8/20\n",
            "Train_loss : 0.27561062574386597\n",
            "----------\n",
            "Epochs 9/20\n",
            "Train_loss : 0.22048798203468323\n",
            "----------\n",
            "Epochs 10/20\n",
            "Train_loss : 0.18693402409553528\n",
            "----------\n",
            "Epochs 11/20\n",
            "Train_loss : 0.1536802500486374\n",
            "----------\n",
            "Epochs 12/20\n",
            "Train_loss : 0.1600993573665619\n",
            "----------\n",
            "Epochs 13/20\n",
            "Train_loss : 0.19075573980808258\n",
            "----------\n",
            "Epochs 14/20\n",
            "Train_loss : 0.23639197647571564\n",
            "----------\n",
            "Epochs 15/20\n",
            "Train_loss : 0.08616496622562408\n",
            "----------\n",
            "Epochs 16/20\n",
            "Train_loss : 0.257250040769577\n",
            "----------\n",
            "Epochs 17/20\n",
            "Train_loss : 0.05961541086435318\n",
            "----------\n",
            "Epochs 18/20\n",
            "Train_loss : 0.1268097460269928\n",
            "----------\n",
            "Epochs 19/20\n",
            "Train_loss : 0.1560366004705429\n",
            "----------\n",
            "Epochs 20/20\n",
            "Train_loss : 0.04730371758341789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "accuracy = 0\n",
        "\n",
        "for data, label in test_dataloader:\n",
        "  data, label = data.to(device), label.to(device)\n",
        "  output = torch.argmax(model(data), dim=1)\n",
        "  accuracy += (output == label).sum().item()\n",
        "\n",
        "print(accuracy / len(test_dataloader.dataset) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSwpOdvWAf2t",
        "outputId": "6911b9ed-b07b-4165-a16a-9ed570cfc368"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97.13000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet"
      ],
      "metadata": {
        "id": "vFd3wxD8C_vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.stride = stride\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def down_sampling(self, x):\n",
        "        out = F.pad(x, (0, 0, 0, 0, 0, self.out_channels - self.in_channels))\n",
        "        out = nn.MaxPool2d(2, stride=self.stride)(out)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x # this will be used to build residual connection.\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.stride != 1:\n",
        "            shortcut = self.down_sampling(x)\n",
        "\n",
        "        out += shortcut # residual connection\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "mVKt0KbMHZDc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_layers, block, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        #input(channel:3) -> (conv 3x3) -> (bn) -> (relu) -> output(channel:16)\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # feature map size = 16x32x32\n",
        "        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n",
        "        # feature map size = 32x16x16\n",
        "        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n",
        "        # feature map size = 64x8x8\n",
        "        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n",
        "\n",
        "        # output layers\n",
        "        self.avg_pool = nn.AvgPool2d(8, stride=1)\n",
        "        self.fc_out = nn.Linear(64, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "    def get_layers(self, block, in_channels, out_channels, stride):\n",
        "        if stride == 2:\n",
        "            down_sample = True\n",
        "        else:\n",
        "            down_sample = False\n",
        "\n",
        "        layer_list = nn.ModuleList([block(in_channels, out_channels, stride, down_sample)])\n",
        "\n",
        "        for _ in range(self.num_layers - 1):\n",
        "            layer_list.append(block(out_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layer_list)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layers_2n(x)\n",
        "        x = self.layers_4n(x)\n",
        "        x = self.layers_6n(x)\n",
        "\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_out(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ovomzhKNDBf3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet18():\n",
        "    block = ResidualBlock\n",
        "    model = ResNet(3, block)\n",
        "    return model\n",
        "def resnet32():\n",
        "    block = ResidualBlock\n",
        "    model = ResNet(5, block)\n",
        "    return model"
      ],
      "metadata": {
        "id": "NcgyyQoJNmT9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = resnet18().to(device)\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "decay_epoch = [32000, 48000]\n",
        "step_lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=decay_epoch, gamma=0.1)"
      ],
      "metadata": {
        "id": "yrpuzRr7Nos6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "net.train()\n",
        "\n",
        "step = 0\n",
        "epochs = 0\n",
        "losses = []\n",
        "\n",
        "while step < 64000:\n",
        "\n",
        "    train_loss = 0.0\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(cifar_loader['train']):\n",
        "        step += 1\n",
        "        step_lr_scheduler.step()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        out = net(x)\n",
        "        loss = criterion(out, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        correct += (out.argmax(1) == y).float().sum().item()\n",
        "        total += x.size(0)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    losses.append(train_loss)\n",
        "    epochs += 1\n",
        "\n",
        "    print(\"Epoch[{:d} ({:d}/64000) ({:.4f}sec)] loss: {:.2f} acc: {:.2f}\".format(epochs, step, time.time()-start_time, train_loss, 100.*correct/total))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_jC2NTxzNsxW",
        "outputId": "2dd609b9-6d28-4caf-b4a1-eb536cfcc2c8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[1 (391/64000) (22.6064sec)] loss: 659.64 acc: 38.45\n",
            "Epoch[2 (782/64000) (45.1633sec)] loss: 426.82 acc: 61.09\n",
            "Epoch[3 (1173/64000) (67.8785sec)] loss: 336.28 acc: 69.76\n",
            "Epoch[4 (1564/64000) (89.7628sec)] loss: 293.58 acc: 73.67\n",
            "Epoch[5 (1955/64000) (112.2177sec)] loss: 265.68 acc: 76.26\n",
            "Epoch[6 (2346/64000) (135.1166sec)] loss: 244.22 acc: 78.14\n",
            "Epoch[7 (2737/64000) (157.5045sec)] loss: 227.42 acc: 79.90\n",
            "Epoch[8 (3128/64000) (180.1812sec)] loss: 214.10 acc: 81.04\n",
            "Epoch[9 (3519/64000) (202.7403sec)] loss: 205.63 acc: 81.67\n",
            "Epoch[10 (3910/64000) (224.2537sec)] loss: 193.49 acc: 83.09\n",
            "Epoch[11 (4301/64000) (247.0863sec)] loss: 187.82 acc: 83.21\n",
            "Epoch[12 (4692/64000) (269.7395sec)] loss: 180.14 acc: 84.14\n",
            "Epoch[13 (5083/64000) (292.2314sec)] loss: 174.57 acc: 84.65\n",
            "Epoch[14 (5474/64000) (314.2126sec)] loss: 168.02 acc: 85.06\n",
            "Epoch[15 (5865/64000) (337.0401sec)] loss: 162.11 acc: 85.50\n",
            "Epoch[16 (6256/64000) (359.8452sec)] loss: 158.38 acc: 85.84\n",
            "Epoch[17 (6647/64000) (381.5541sec)] loss: 154.62 acc: 86.25\n",
            "Epoch[18 (7038/64000) (405.2664sec)] loss: 151.26 acc: 86.76\n",
            "Epoch[19 (7429/64000) (427.6263sec)] loss: 148.95 acc: 86.78\n",
            "Epoch[20 (7820/64000) (449.7589sec)] loss: 146.43 acc: 87.03\n",
            "Epoch[21 (8211/64000) (471.5953sec)] loss: 144.84 acc: 87.18\n",
            "Epoch[22 (8602/64000) (495.5380sec)] loss: 139.50 acc: 87.59\n",
            "Epoch[23 (8993/64000) (519.5606sec)] loss: 136.39 acc: 87.73\n",
            "Epoch[24 (9384/64000) (542.8068sec)] loss: 136.13 acc: 87.92\n",
            "Epoch[25 (9775/64000) (564.4557sec)] loss: 131.49 acc: 88.29\n",
            "Epoch[26 (10166/64000) (586.9589sec)] loss: 130.70 acc: 88.37\n",
            "Epoch[27 (10557/64000) (609.2182sec)] loss: 128.87 acc: 88.41\n",
            "Epoch[28 (10948/64000) (630.7252sec)] loss: 127.07 acc: 88.60\n",
            "Epoch[29 (11339/64000) (655.0744sec)] loss: 125.43 acc: 88.78\n",
            "Epoch[30 (11730/64000) (679.5472sec)] loss: 123.81 acc: 88.96\n",
            "Epoch[31 (12121/64000) (702.7206sec)] loss: 121.86 acc: 89.09\n",
            "Epoch[32 (12512/64000) (725.3151sec)] loss: 120.26 acc: 89.26\n",
            "Epoch[33 (12903/64000) (747.0272sec)] loss: 119.17 acc: 89.40\n",
            "Epoch[34 (13294/64000) (770.1211sec)] loss: 119.56 acc: 89.34\n",
            "Epoch[35 (13685/64000) (792.9503sec)] loss: 117.94 acc: 89.44\n",
            "Epoch[36 (14076/64000) (815.8666sec)] loss: 116.44 acc: 89.66\n",
            "Epoch[37 (14467/64000) (837.3609sec)] loss: 113.70 acc: 89.80\n",
            "Epoch[38 (14858/64000) (859.8043sec)] loss: 114.32 acc: 89.85\n",
            "Epoch[39 (15249/64000) (882.0580sec)] loss: 112.31 acc: 89.96\n",
            "Epoch[40 (15640/64000) (903.3694sec)] loss: 111.70 acc: 90.00\n",
            "Epoch[41 (16031/64000) (926.7270sec)] loss: 111.82 acc: 90.00\n",
            "Epoch[42 (16422/64000) (948.9284sec)] loss: 110.52 acc: 90.05\n",
            "Epoch[43 (16813/64000) (970.1830sec)] loss: 109.04 acc: 90.25\n",
            "Epoch[44 (17204/64000) (992.5258sec)] loss: 108.37 acc: 90.28\n",
            "Epoch[45 (17595/64000) (1015.3880sec)] loss: 106.85 acc: 90.49\n",
            "Epoch[46 (17986/64000) (1036.6851sec)] loss: 106.36 acc: 90.44\n",
            "Epoch[47 (18377/64000) (1059.1172sec)] loss: 105.20 acc: 90.58\n",
            "Epoch[48 (18768/64000) (1081.7434sec)] loss: 105.68 acc: 90.38\n",
            "Epoch[49 (19159/64000) (1103.4905sec)] loss: 102.77 acc: 90.75\n",
            "Epoch[50 (19550/64000) (1125.5257sec)] loss: 102.50 acc: 90.87\n",
            "Epoch[51 (19941/64000) (1147.9379sec)] loss: 102.87 acc: 90.86\n",
            "Epoch[52 (20332/64000) (1170.2041sec)] loss: 102.02 acc: 90.95\n",
            "Epoch[53 (20723/64000) (1193.0818sec)] loss: 99.92 acc: 91.00\n",
            "Epoch[54 (21114/64000) (1215.5913sec)] loss: 98.34 acc: 91.18\n",
            "Epoch[55 (21505/64000) (1237.7293sec)] loss: 98.83 acc: 91.24\n",
            "Epoch[56 (21896/64000) (1260.6622sec)] loss: 102.05 acc: 90.94\n",
            "Epoch[57 (22287/64000) (1282.7127sec)] loss: 98.52 acc: 91.36\n",
            "Epoch[58 (22678/64000) (1305.6756sec)] loss: 98.15 acc: 91.24\n",
            "Epoch[59 (23069/64000) (1328.7409sec)] loss: 99.28 acc: 91.10\n",
            "Epoch[60 (23460/64000) (1351.2900sec)] loss: 100.49 acc: 90.96\n",
            "Epoch[61 (23851/64000) (1373.6661sec)] loss: 95.74 acc: 91.53\n",
            "Epoch[62 (24242/64000) (1396.0717sec)] loss: 96.10 acc: 91.33\n",
            "Epoch[63 (24633/64000) (1418.4303sec)] loss: 95.22 acc: 91.38\n",
            "Epoch[64 (25024/64000) (1439.7892sec)] loss: 96.18 acc: 91.39\n",
            "Epoch[65 (25415/64000) (1463.1686sec)] loss: 95.63 acc: 91.54\n",
            "Epoch[66 (25806/64000) (1485.4709sec)] loss: 94.45 acc: 91.50\n",
            "Epoch[67 (26197/64000) (1507.3054sec)] loss: 95.01 acc: 91.47\n",
            "Epoch[68 (26588/64000) (1528.5890sec)] loss: 95.37 acc: 91.59\n",
            "Epoch[69 (26979/64000) (1550.7966sec)] loss: 94.31 acc: 91.60\n",
            "Epoch[70 (27370/64000) (1572.8259sec)] loss: 92.39 acc: 91.63\n",
            "Epoch[71 (27761/64000) (1594.3813sec)] loss: 94.01 acc: 91.66\n",
            "Epoch[72 (28152/64000) (1616.6645sec)] loss: 92.37 acc: 91.70\n",
            "Epoch[73 (28543/64000) (1638.6841sec)] loss: 91.91 acc: 91.70\n",
            "Epoch[74 (28934/64000) (1659.7044sec)] loss: 91.98 acc: 91.74\n",
            "Epoch[75 (29325/64000) (1681.9454sec)] loss: 92.27 acc: 91.55\n",
            "Epoch[76 (29716/64000) (1703.9702sec)] loss: 91.85 acc: 91.75\n",
            "Epoch[77 (30107/64000) (1725.3665sec)] loss: 88.41 acc: 92.11\n",
            "Epoch[78 (30498/64000) (1749.2090sec)] loss: 90.75 acc: 91.88\n",
            "Epoch[79 (30889/64000) (1771.8399sec)] loss: 89.02 acc: 91.97\n",
            "Epoch[80 (31280/64000) (1794.5406sec)] loss: 91.19 acc: 91.73\n",
            "Epoch[81 (31671/64000) (1815.8912sec)] loss: 90.46 acc: 91.87\n",
            "Epoch[82 (32062/64000) (1838.4319sec)] loss: 86.81 acc: 92.15\n",
            "Epoch[83 (32453/64000) (1860.8064sec)] loss: 56.59 acc: 95.12\n",
            "Epoch[84 (32844/64000) (1882.1152sec)] loss: 47.26 acc: 95.96\n",
            "Epoch[85 (33235/64000) (1904.7777sec)] loss: 43.86 acc: 96.23\n",
            "Epoch[86 (33626/64000) (1927.3562sec)] loss: 41.10 acc: 96.35\n",
            "Epoch[87 (34017/64000) (1948.7944sec)] loss: 38.69 acc: 96.64\n",
            "Epoch[88 (34408/64000) (1970.5469sec)] loss: 36.06 acc: 96.95\n",
            "Epoch[89 (34799/64000) (1992.8078sec)] loss: 35.82 acc: 96.93\n",
            "Epoch[90 (35190/64000) (2016.2691sec)] loss: 34.65 acc: 97.04\n",
            "Epoch[91 (35581/64000) (2037.4093sec)] loss: 33.40 acc: 97.10\n",
            "Epoch[92 (35972/64000) (2059.3928sec)] loss: 32.37 acc: 97.20\n",
            "Epoch[93 (36363/64000) (2081.5976sec)] loss: 31.00 acc: 97.31\n",
            "Epoch[94 (36754/64000) (2102.8946sec)] loss: 29.62 acc: 97.53\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0e016e6e888b>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mstep_lr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-2ede391d6775>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers_2n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers_4n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers_6n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-048b8c4e359a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mshortcut\u001b[0m \u001b[0;31m# residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1456\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}