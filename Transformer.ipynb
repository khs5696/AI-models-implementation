{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khs5696/AI504/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL89lUDx-5Pb"
      },
      "source": [
        "# [AI 504] Programming for AI, Fall 2021\n",
        "# Practice 10: Transformers\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okJXS1-OEbN7"
      },
      "source": [
        "#### [Notifications]\n",
        "- If you have any questions, feel free to ask\n",
        "- For additional questions, send emails: yeonsu.k@kaist.ac.kr    \n",
        "      \n",
        "\n",
        "     \n",
        "     \n",
        "# Table of contents\n",
        "1. [Prepare input](#1)\n",
        "2. [Implement Transformer](#2)\n",
        "3. [Train and Evaluate](#3)\n",
        "4. [Visualize attention](#4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2Bj-rF9EbN7"
      },
      "source": [
        "# Prepare essential packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHrI30vIEbN7",
        "outputId": "afa76510-e4fd-49c7-e7f6-6800235f4bf2",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'attentionviz'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 30 (delta 10), reused 19 (delta 4), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (30/30), 19.54 KiB | 2.44 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n",
            "2023-11-22 08:22:17.239845: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-22 08:22:17.239901: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-22 08:22:17.239939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-22 08:22:17.247741: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-22 08:22:18.381421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-22 08:22:19.818023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-22 08:22:19.818493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-22 08:22:19.818670: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
            "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
            "Collecting de-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.6.0/de_core_news_sm-3.6.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "2023-11-22 08:22:42.282388: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-22 08:22:42.282442: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-22 08:22:42.282476: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-22 08:22:42.290030: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-22 08:22:43.392374: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-22 08:22:44.806159: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-22 08:22:44.806548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-22 08:22:44.806714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "!git clone https://github.com/sjpark9503/attentionviz.git\n",
        "!python -m spacy download de\n",
        "!python -m spacy download en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sayXpp8FEbN8"
      },
      "source": [
        "# I. Prepare input\n",
        "<a id='1'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSJg1KusLTL9",
        "outputId": "4f204b98-00ef-432d-f07d-30addf7263f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'multi30k-datase'...\n",
            "remote: Enumerating objects: 313, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 313 (delta 17), reused 21 (delta 16), pack-reused 281\u001b[K\n",
            "Receiving objects: 100% (313/313), 18.21 MiB | 18.21 MiB/s, done.\n",
            "Resolving deltas: 100% (69/69), done.\n",
            "Submodule 'scripts/subword-nmt' (https://github.com/rsennrich/subword-nmt.git) registered for path 'scripts/subword-nmt'\n",
            "Cloning into '/content/multi30k-datase/scripts/subword-nmt'...\n",
            "remote: Enumerating objects: 597, done.        \n",
            "remote: Counting objects: 100% (21/21), done.        \n",
            "remote: Compressing objects: 100% (17/17), done.        \n",
            "remote: Total 597 (delta 8), reused 12 (delta 4), pack-reused 576        \n",
            "Receiving objects: 100% (597/597), 252.23 KiB | 2.36 MiB/s, done.\n",
            "Resolving deltas: 100% (357/357), done.\n",
            "Submodule path 'scripts/subword-nmt': checked out '80b7c1449e2e26673fb0b5cae993fe2d0dc23846'\n"
          ]
        }
      ],
      "source": [
        "!git clone --recursive https://github.com/multi30k/dataset.git multi30k-datase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCQhApJpMavz"
      },
      "outputs": [],
      "source": [
        "!find multi30k-datase/ -name '*.gz' -exec gunzip {} \\;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KWOpLYfEbN8"
      },
      "source": [
        "We've already learned how to preprocess the text data in previous lectures.\n",
        "\n",
        "You can see some detailed explanation about translation datasets in [torchtext](https://pytorch.org/text/), [practice session,week 9](https://classum.com/main/course/7726/103) and [PyTorch NMT tutorial](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaduS25kEbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import os\n",
        "import io\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Load spaCy models for tokenization\n",
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Tokenization function for German and English\n",
        "def tokenize_de(text):\n",
        "    return [tok.text.lower() for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "# Define the custom Dataset class\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, root_dir, split):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "\n",
        "        self.data_files = {\n",
        "            'train': ('train.de', 'train.en'),\n",
        "            'valid': ('val.de', 'val.en'),\n",
        "            'test': ('test_2016_flickr.de', 'test_2016_flickr.en')\n",
        "        }\n",
        "\n",
        "        self.de_file_path = os.path.join(self.root_dir, self.data_files[self.split][0])\n",
        "        self.en_file_path = os.path.join(self.root_dir, self.data_files[self.split][1])\n",
        "\n",
        "        with io.open(self.de_file_path, mode='r', encoding='utf-8') as de_file, \\\n",
        "             io.open(self.en_file_path, mode='r', encoding='utf-8') as en_file:\n",
        "            self.de_sentences = de_file.readlines()\n",
        "            self.en_sentences = en_file.readlines()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.de_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        de_sentence = tokenize_de(self.de_sentences[idx].strip())\n",
        "        en_sentence = tokenize_en(self.en_sentences[idx].strip())\n",
        "        return {'SRC': de_sentence, 'TRG': en_sentence}\n",
        "\n",
        "# Define the Vocab class\n",
        "class Vocab:\n",
        "    def __init__(self, counter, min_freq):\n",
        "        self.itos = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
        "        self.stoi = {token: i for i, token in enumerate(self.itos)}\n",
        "        self.min_freq = min_freq\n",
        "        self.build_vocab(counter)\n",
        "\n",
        "    def build_vocab(self, counter):\n",
        "        for word, freq in counter.items():\n",
        "            if freq >= self.min_freq and word not in self.stoi:\n",
        "                self.stoi[word] = len(self.itos)\n",
        "                self.itos.append(word)\n",
        "\n",
        "    def numericalize(self, tokens):\n",
        "        return [self.stoi.get(token, self.stoi['<unk>']) for token in tokens]\n",
        "\n",
        "# Function to build a counter of words from the dataset\n",
        "def build_counter(dataset):\n",
        "    counter = Counter()\n",
        "    for i in range(len(dataset)):\n",
        "        example = dataset[i]\n",
        "        counter.update(example['SRC'])\n",
        "        counter.update(example['TRG'])\n",
        "    return counter\n",
        "\n",
        "# Create the datasets\n",
        "train_data = TranslationDataset(root_dir='/content/multi30k-datase/data/task1/raw', split='train')\n",
        "valid_data = TranslationDataset(root_dir='/content/multi30k-datase/data/task1/raw', split='valid')\n",
        "test_data = TranslationDataset(root_dir='/content/multi30k-datase/data/task1/raw', split='test')\n",
        "\n",
        "# Build the counter and vocabularies\n",
        "counter = build_counter(train_data)\n",
        "SRC_vocab = Vocab(counter, min_freq=2)\n",
        "TRG_vocab = Vocab(counter, min_freq=2)\n",
        "\n",
        "# Define the collate function to process batches\n",
        "def collate_fn(batch):\n",
        "    src_batch = [torch.tensor(SRC_vocab.numericalize(item['SRC'])) for item in batch]\n",
        "    trg_batch = [torch.tensor(TRG_vocab.numericalize(item['TRG'])) for item in batch]\n",
        "\n",
        "    src_batch_padded = pad_sequence(src_batch, padding_value=SRC_vocab.stoi['<pad>'], batch_first=True)\n",
        "    trg_batch_padded = pad_sequence(trg_batch, padding_value=TRG_vocab.stoi['<pad>'], batch_first=True)\n",
        "\n",
        "    return {'SRC': src_batch_padded, 'TRG': trg_batch_padded}\n",
        "\n",
        "\n",
        "# Set device for DataLoader\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define batch size\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "\n",
        "train_iterator = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "valid_iterator = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "test_iterator = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE_u1Qg-EbN8"
      },
      "source": [
        "# II. Implement Transformer\n",
        "<a id='2'></a>\n",
        "In practice 10, we will learn how to implement the __[Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) (Vaswani et al., 2017)__\n",
        "\n",
        "The overall architecutre is as follows:\n",
        "<div>\n",
        "<img src=\"http://incredible.ai/assets/images/transformer-architecture.png\" width=400)/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqEVVfl-EbN8"
      },
      "source": [
        "## 1. Basic building blocks\n",
        "\n",
        "In this sections, we will build blocks of the transformer: [Multi-head attention](#1a), [Position wise feedforward network](#1b) and [Positional encoding](#1c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeI2oINrEbN8"
      },
      "source": [
        "### a. Attention\n",
        "<a id='1a'></a>\n",
        "In this section, you will implement scaled dot-product attention and multi-head attention.\n",
        "\n",
        "__Scaled dot product:__\n",
        "![picture](http://incredible.ai/assets/images/transformer-scaled-dot-product.png)\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "\n",
        "__Multi-head attention:__\n",
        "<img src=\"http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png\" width=650)/>\n",
        "* Equation:\n",
        "$$\\begin{align} \\text{MultiHead}(Q, K, V) &= \\text{Concat}(head_1, ...., head_h) W^O \\\\\n",
        "\\text{where head}_i &= \\text{Attention} \\left( QW^Q_i, K W^K_i, VW^v_i \\right)\n",
        "\\end{align}$$\n",
        "</div>\n",
        "\n",
        "<div>\n",
        "\n",
        "__Query, Key and Value projection:__\n",
        "<img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation.png\" width=400)/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07AkqQcqEbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  Self-attention의 목적 : 입력 문장 내 단어들 간의 유사도를 구하기 위해\n",
        "  Q, K, V : 입력 문장의 모든 단어 벡터들\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        emb_dim,\n",
        "        num_heads,\n",
        "        dropout=0.0,\n",
        "        bias=False,\n",
        "        encoder_decoder_attention=False,  # otherwise self_attention\n",
        "        causal = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = emb_dim // num_heads\n",
        "        assert self.head_dim * num_heads == self.emb_dim, \"emb_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.encoder_decoder_attention = encoder_decoder_attention\n",
        "        self.causal = causal\n",
        "        self.q_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
        "        self.k_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
        "        self.out_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (\n",
        "            self.num_heads,\n",
        "            self.head_dim,\n",
        "        )\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "        # This is equivalent to\n",
        "        # return x.transpose(1,2)\n",
        "\n",
        "    '''\n",
        "      각 Q 벡터는 모든 K 벡터에 대해 attention score 및 분포(weight)를 구하고\n",
        "      모든 V 벡터를 가중합하여 Attention Value (Context vector)를 계산한다.\n",
        "    '''\n",
        "    def scaled_dot_product(self,\n",
        "                           query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
        "                           attention_mask: torch.BoolTensor):\n",
        "        attn_weights = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(self.emb_dim) # QK^T/sqrt(d)\n",
        "\n",
        "        # Padding Mask : Key의 경우 <PAD> 토큰의 유사도 계산을 제외(무시)해야 하기 때문에\n",
        "        # Attention score matrix의 마스킹 위치에 매우 작은 음수값(-INF)을 넣어준다. → Softmax에 의해 0으로 변환\n",
        "        if attention_mask is not None:\n",
        "            attn_weights = attn_weights.masked_fill(attention_mask.unsqueeze(1), float(\"-inf\"))\n",
        "\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)    # softmax(QK^T/sqrt(d))\n",
        "        attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "        attn_output = torch.matmul(attn_probs, value)     # softmax(QK^T/sqrt(d))V → Attention Value >> seq_len x (d_model/num_heads)\n",
        "\n",
        "        return attn_output, attn_probs\n",
        "\n",
        "    '''\n",
        "      여러 번의 attention을 num_heads만큼 병렬로 수행한다. ← 다양한 시각으로 정보를 수집하기 위해\n",
        "      수행된 병렬 attention은 하나로 연결되고(concatenate) 가중치 행렬과 곱해진다. → (seq_len x d_model)\n",
        "    '''\n",
        "    # Attention score matrix 계산 : dot production(Q & K) + scaling\n",
        "    # Query, Key, Value : (batch_size, num_heads, seq_len, (d_model/num_heads))\n",
        "    def MultiHead_scaled_dot_product(self,\n",
        "                       query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
        "                       attention_mask: torch.BoolTensor):\n",
        "        # attn_weights : (batch_size, num_heads, seq_len, seq_len)\n",
        "        attn_weights = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(self.head_dim) # QK^T/sqrt(d)\n",
        "\n",
        "\n",
        "        # Attention mask\n",
        "        if attention_mask is not None:\n",
        "            if self.causal:\n",
        "              # (seq_len x seq_len)\n",
        "                attn_weights = attn_weights.masked_fill(attention_mask.unsqueeze(0).unsqueeze(1), float(\"-inf\"))\n",
        "            else:\n",
        "              # (batch_size x seq_len)\n",
        "                attn_weights = attn_weights.masked_fill(attention_mask.unsqueeze(1).unsqueeze(2), float(\"-inf\"))\n",
        "\n",
        "\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)               # softmax(QK^T/sqrt(d))\n",
        "        attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "\n",
        "        # softmax(QK^T/sqrt(d))V → Attention Value\n",
        "        attn_output = torch.matmul(attn_probs, value)                          # (batch_size, num_heads, seq_len, (d_model/num_heads)) : (128, 4, 27, 16)\n",
        "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()             # (batch_size, seq_len, num_heads, (d_model/num_heads)) : (128, 27, 4, 16)\n",
        "        concat_attn_output_shape = attn_output.size()[:-2] + (self.emb_dim,)   #\n",
        "        attn_output = attn_output.view(*concat_attn_output_shape)              # (batch_size, seq_len, emb_dim) : (128, 27, 64)\n",
        "        attn_output = self.out_proj(attn_output)                               # (batch_size, seq_len, emb_dim) : (128, 27, 64)\n",
        "\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "    def forward(self, query: torch.Tensor, key: torch.Tensor, attention_mask: torch.Tensor = None, ):\n",
        "        # 각 단어 벡터(d_model = 512)로부터 Q, K, V(d = d_model/num_heads = 64)를 얻기 위해 각 가중치 행렬을 곱한다.\n",
        "        # 가중치 행렬(q_proj, k_proj, v_proj) : d_model x d_model\n",
        "        q = self.q_proj(query)          # query : (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Enc-Dec attention\n",
        "        if self.encoder_decoder_attention:\n",
        "            k = self.k_proj(key)\n",
        "            v = self.v_proj(key)\n",
        "        # Self attention\n",
        "        else:\n",
        "            k = self.k_proj(query)\n",
        "            v = self.v_proj(query)\n",
        "\n",
        "        q = self.transpose_for_scores(q)  # q : (batch_size, num_heads, seq_len, (d_model/num_heads))\n",
        "        k = self.transpose_for_scores(k)\n",
        "        v = self.transpose_for_scores(v)\n",
        "        # q : (batch_size, num_heads, seq_len, (d_model/num_heads))\n",
        "        attn_output, attn_weights = self.MultiHead_scaled_dot_product(q,k,v,attention_mask)\n",
        "        return attn_output, attn_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b528gtwHEbN8"
      },
      "source": [
        "### b. Position-wise feed-forward network\n",
        "<a id='1b'></a>\n",
        "In this section, we will implement position-wise feed forward network\n",
        "\n",
        "$$\\text{FFN}(x) = \\max \\left(0, x W_1 + b_1 \\right) W_2 + b_2$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBqWWdIyEbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  하나의 인코더 층 내에서는 각 단어에 대해 동일하게 사용되지만,\n",
        "  인코더 층마다는 다른 값을 가진다.\n",
        "'''\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim: int, d_ff: int, dropout: float = 0.1):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "        self.w_1 = nn.Linear(emb_dim, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, emb_dim)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x):                     # input(x) : (seq_len, d_model = emb_dim)\n",
        "        residual = x\n",
        "        x = self.activation(self.w_1(x))      # (seq_len, d_ff)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.w_2(x)                       # (seq_len, d_model)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x + residual                   # residual connection for preventing gradient vanishing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9-qkoUKEbN8"
      },
      "source": [
        "### c. Sinusoidal Positional Encoding\n",
        "<a id='1c'></a>\n",
        "In this section, we will implement sinusoidal positional encoding\n",
        "\n",
        "$$\\begin{align}\n",
        "PE(pos, 2i) &= \\sin \\left( pos / 10000^{2i / d_{model}} \\right)  \\\\\n",
        "PE(pos, 2i+1) &= \\cos \\left( pos / 10000^{2i / d_{model}} \\right)  \n",
        "\\end{align}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsiJalEvEbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Since Transformer contains no recurrence and no convolution,\n",
        "# in order for the model to make use of the order of the sequence,\n",
        "# we must inject some information about the relative or absolute position of the tokens in the sequence.\n",
        "# To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks.\n",
        "# There are many choices of positional encodings, learned and fixed\n",
        "\n",
        "'''\n",
        "   Positional Encoding : Transformer는 단어 입력을 순차적으로 받는 방식이 아니기 때문에\n",
        "  각 단어의 embedding vector에 위치 정보를 더하여 모델의 입력으로 사용한다.\n",
        "'''\n",
        "\n",
        "class SinusoidalPositionalEmbedding(nn.Embedding):\n",
        "    def __init__(self, num_positions, embedding_dim, padding_idx=None):\n",
        "        super().__init__(num_positions, embedding_dim)  # torch.nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.weight = self._init_weight(self.weight)    # self.weight => nn.Embedding(num_positions, embedding_dim).weight\n",
        "\n",
        "    @staticmethod\n",
        "    # input(out) : n_pos개의 단어에 대한 각각의 embedding vector : (n_pos, embed_dim)\n",
        "    def _init_weight(out: nn.Parameter):\n",
        "        n_pos, embed_dim = out.shape\n",
        "        pe = nn.Parameter(torch.zeros(out.shape))\n",
        "        # Embedding vector(pos) 내의 각 차원 인덱스(i)가 짝수인 경우 sin 함수로, 홀수인 경우 cos 함수의 값으로 보정한다.\n",
        "        for pos in range(n_pos):\n",
        "            for i in range(0, embed_dim, 2):\n",
        "                pe[pos, i].data.copy_( torch.tensor( np.sin(pos / (10000 ** ( i / embed_dim)))) )\n",
        "                pe[pos, i + 1].data.copy_( torch.tensor( np.cos(pos / (10000 ** ((i + 1) / embed_dim)))) )\n",
        "        pe.detach_()\n",
        "\n",
        "        return pe\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, input_ids):\n",
        "      bsz, seq_len = input_ids.shape[:2]\n",
        "      positions = torch.arange(seq_len, dtype=torch.long, device=self.weight.device)\n",
        "      return super().forward(positions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdhwI3hPEbN8"
      },
      "source": [
        "## 2. Transformer Encoder\n",
        "\n",
        "Now we have all basic building blocks which are essential to build Transformer.\n",
        "\n",
        "Let's implement Transformer step-by-step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6ym2hKzEbN8"
      },
      "source": [
        "### a. Encoder layer\n",
        "In this section, we will implement single layer of Transformer encoder.\n",
        "<div>\n",
        "<img src=\"https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm\" width=200)/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6B93kjUlEbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.emb_dim = config.emb_dim\n",
        "        self.ffn_dim = config.ffn_dim\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(\n",
        "            emb_dim=self.emb_dim,                 # emb_dim = 64\n",
        "            num_heads=config.attention_heads,     # num_heads = 4\n",
        "            dropout=config.attention_dropout)\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.emb_dim)\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = nn.ReLU()\n",
        "\n",
        "        self.PositionWiseFeedForward = PositionWiseFeedForward(self.emb_dim, self.ffn_dim, config.dropout)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.emb_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x, encoder_padding_mask):\n",
        "        residual = x\n",
        "        x, attn_weights = self.self_attn(query=x, key=x, attention_mask=encoder_padding_mask)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        x = self.self_attn_layer_norm(x)\n",
        "\n",
        "        x = self.PositionWiseFeedForward(x)\n",
        "        x = self.final_layer_norm(x)\n",
        "\n",
        "        if torch.isinf(x).any() or torch.isnan(x).any():\n",
        "            clamp_value = torch.finfo(x.dtype).max - 1000\n",
        "            x = torch.clamp(x, min=-clamp_value, max=clamp_value)\n",
        "\n",
        "        return x, attn_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LygNGzM0EbN8"
      },
      "source": [
        "### b. Encoder\n",
        "\n",
        "Stack encoder layers and build full Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZOAlAv7EbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    # embed_tokens의 실제 입력값 : nn.Embedding(len(SRC_vocab.itos), config.emb_dim, padding_idx=SRC_vocab.stoi['<pad>'])\n",
        "    # len(SRC_vocab.itos) x config.emb_dim 크기의 lookup table\n",
        "    def __init__(self, config, embed_tokens):\n",
        "        super().__init__()\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "        emb_dim = embed_tokens.embedding_dim\n",
        "        self.padding_idx = embed_tokens.padding_idx\n",
        "        self.max_source_positions = config.max_position_embeddings\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.embed_tokens = embed_tokens\n",
        "        self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "                config.max_position_embeddings, config.emb_dim, self.padding_idx\n",
        "            )\n",
        "\n",
        "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):  # input_ids : (batch_size, seq_len)\n",
        "        ''' Embedding & Positional Encoding '''\n",
        "        inputs_embeds = self.embed_tokens(input_ids)    # input_embeds : (batch_size, seq_len, emb_dim)\n",
        "        embed_pos = self.embed_positions(input_ids)     # embed_pos : (seq_len, emb_dim)\n",
        "        x = inputs_embeds + embed_pos                   # x : (batch_size, seq_len, emb_dim) (128, 23, 64)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        ''' Multi-head self-attention '''\n",
        "        self_attn_scores = []\n",
        "        for encoder_layer in self.layers:\n",
        "            x, attn = encoder_layer(x, attention_mask)\n",
        "            self_attn_scores.append(attn.detach())\n",
        "\n",
        "        return x, self_attn_scores    # (batch_size, seq_len, emb_dim), (batch_size, num_heads, seq_len, seq_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgjqDJnKEbN8"
      },
      "source": [
        "## 3. Transformer Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73LEB0mBEbN8"
      },
      "source": [
        "### a.Decoder layer\n",
        "In this section, we will implement single layer of Transformer decoder.\n",
        "<div>\n",
        "<img src=\"http://incredible.ai/assets/images/transformer-decoder.png\" width=180)/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HgMu2QCEbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.emb_dim = config.emb_dim\n",
        "        self.ffn_dim = config.ffn_dim\n",
        "        self.self_attn = MultiHeadAttention(\n",
        "            emb_dim=self.emb_dim,\n",
        "            num_heads=config.attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            causal=True,\n",
        "        )\n",
        "        self.dropout = config.dropout\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.emb_dim)\n",
        "        self.encoder_attn = MultiHeadAttention(\n",
        "            emb_dim=self.emb_dim,\n",
        "            num_heads=config.attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            encoder_decoder_attention=True,\n",
        "        )\n",
        "        self.encoder_attn_layer_norm = nn.LayerNorm(self.emb_dim)\n",
        "        self.PositionWiseFeedForward = PositionWiseFeedForward(self.emb_dim, self.ffn_dim, config.dropout)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.emb_dim)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        encoder_hidden_states,\n",
        "        encoder_attention_mask=None,\n",
        "        causal_mask=None,\n",
        "    ):\n",
        "        residual = x\n",
        "        # Self Attention\n",
        "        x, self_attn_weights = self.self_attn(\n",
        "            query=x,\n",
        "            key=x, # adds keys to layer state\n",
        "            attention_mask=causal_mask,\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        x = self.self_attn_layer_norm(x)\n",
        "\n",
        "        # Cross-Attention Block\n",
        "        residual = x\n",
        "        x, cross_attn_weights = self.encoder_attn(\n",
        "            query=x,\n",
        "            key=encoder_hidden_states,\n",
        "            attention_mask=encoder_attention_mask,\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        x = self.encoder_attn_layer_norm(x)\n",
        "\n",
        "        # Fully Connected\n",
        "        x = self.PositionWiseFeedForward(x)\n",
        "        x = self.final_layer_norm(x)\n",
        "\n",
        "        return (\n",
        "            x,\n",
        "            self_attn_weights,\n",
        "            cross_attn_weights,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAJQ-q5mEbN8"
      },
      "source": [
        "### b. Decoder\n",
        "\n",
        "Stack decoder layers and build full Transformer decoder.\n",
        "\n",
        "Unlike the encoder, you need to do one more job: pass the causal(unidirectional) mask to the decoder self attention layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEMa6owhEbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, config, embed_tokens: nn.Embedding):\n",
        "        super().__init__()\n",
        "        self.dropout = config.dropout\n",
        "        self.padding_idx = embed_tokens.padding_idx\n",
        "        self.max_target_positions = config.max_position_embeddings\n",
        "        self.embed_tokens = embed_tokens\n",
        "        self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "            config.max_position_embeddings, config.emb_dim, self.padding_idx\n",
        "        )\n",
        "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.decoder_layers)])  # type: List[DecoderLayer]\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        encoder_hidden_states,\n",
        "        encoder_attention_mask,\n",
        "        decoder_causal_mask,\n",
        "    ):\n",
        "\n",
        "        # embed positions\n",
        "        positions = self.embed_positions(input_ids)\n",
        "        x = self.embed_tokens(input_ids)\n",
        "        x += positions\n",
        "\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # decoder layers\n",
        "        cross_attention_scores = []\n",
        "        for idx, decoder_layer in enumerate(self.layers):\n",
        "            x, layer_self_attn, layer_cross_attn = decoder_layer(\n",
        "                x,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask=encoder_attention_mask,\n",
        "                causal_mask=decoder_causal_mask,\n",
        "            )\n",
        "            cross_attention_scores.append(layer_cross_attn.detach())\n",
        "\n",
        "        return x, cross_attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr0g3oeIEbN8"
      },
      "source": [
        "## 4. Transformer\n",
        "\n",
        "Let's combine encoder and decoder in one place!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4aZzq8GEbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming 'Encoder' and 'Decoder' are defined elsewhere in your code\n",
        "# If not, you'll need to define these classes as well\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, SRC_vocab, TRG_vocab, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.SRC_vocab = SRC_vocab\n",
        "        self.TRG_vocab = TRG_vocab\n",
        "\n",
        "        self.enc_embedding = nn.Embedding(len(SRC_vocab.itos), config.emb_dim, padding_idx=SRC_vocab.stoi['<pad>'])\n",
        "        self.dec_embedding = nn.Embedding(len(TRG_vocab.itos), config.emb_dim, padding_idx=TRG_vocab.stoi['<pad>'])\n",
        "\n",
        "        self.encoder = Encoder(config, self.enc_embedding)\n",
        "        self.decoder = Decoder(config, self.dec_embedding)\n",
        "\n",
        "        self.prediction_head = nn.Linear(config.emb_dim, len(TRG_vocab.itos))\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def generate_mask(self, src, trg):\n",
        "        # Mask encoder attention to ignore padding\n",
        "        enc_attention_mask = src.eq(self.SRC_vocab.stoi['<pad>']).to(device)\n",
        "        # Mask decoder attention for causality\n",
        "        tmp = torch.ones(trg.size(1), trg.size(1), dtype=torch.bool, device=device)\n",
        "        mask = torch.arange(tmp.size(-1), device=device)\n",
        "        dec_attention_mask = tmp.masked_fill_(mask < (mask + 1).view(tmp.size(-1), 1), False).to(device)\n",
        "\n",
        "        return enc_attention_mask, dec_attention_mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        for name, param in self.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                if 'weight' in name:\n",
        "                    nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "                else:\n",
        "                    nn.init.constant_(param.data, 0)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        enc_attention_mask, dec_causal_mask = self.generate_mask(src, trg)\n",
        "        encoder_output, encoder_attention_scores = self.encoder(\n",
        "            input_ids=src,\n",
        "            attention_mask=enc_attention_mask\n",
        "        )\n",
        "\n",
        "        decoder_output, decoder_attention_scores = self.decoder(\n",
        "            trg,\n",
        "            encoder_output,\n",
        "            encoder_attention_mask=enc_attention_mask,\n",
        "            decoder_causal_mask=dec_causal_mask,\n",
        "        )\n",
        "        decoder_output = self.prediction_head(decoder_output)\n",
        "\n",
        "        return decoder_output, encoder_attention_scores, decoder_attention_scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU-llE39EbN8"
      },
      "source": [
        "# III. Train & Evaluate\n",
        "<a id='3'></a>\n",
        "This section is very similar to week 9, so please refer to it for detailed description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZRMlUmxEbN8"
      },
      "source": [
        "## 1. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlIc_VKaEbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import easydict\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Create the configuration for the transformer model\n",
        "config = easydict.EasyDict({\n",
        "    \"emb_dim\": 64,\n",
        "    \"ffn_dim\": 256,\n",
        "    \"attention_heads\": 4,\n",
        "    \"attention_dropout\": 0.0,\n",
        "    \"dropout\": 0.2,\n",
        "    \"max_position_embeddings\": 512,\n",
        "    \"encoder_layers\": 3,\n",
        "    \"decoder_layers\": 3,\n",
        "})\n",
        "\n",
        "# Constants for training\n",
        "N_EPOCHS = 100\n",
        "learning_rate = 5e-4\n",
        "CLIP = 1\n",
        "\n",
        "# Updated PAD_IDX to use the new Vocab instance\n",
        "PAD_IDX = SRC_vocab.stoi['<pad>']\n",
        "\n",
        "# Instantiate the model using the new Vocab instances instead of the Fields\n",
        "model = Transformer(SRC_vocab, TRG_vocab, config)\n",
        "model.to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define the loss function, ignoring the index of the padding token\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "# Initialize the best validation loss\n",
        "best_valid_loss = float('inf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hql5wOKEbN8"
      },
      "source": [
        "## 2. Train & Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 913
        },
        "id": "F1HHCxXuEbN8",
        "outputId": "f2b91313-02ba-47d6-d7d1-afdab038dc11",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1/100 [00:00<00:15,  6.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query : torch.Size([128, 31, 64])\n",
            "q : torch.Size([128, 4, 31, 16])\n",
            "query : torch.Size([128, 31, 64])\n",
            "q : torch.Size([128, 4, 31, 16])\n",
            "query : torch.Size([128, 31, 64])\n",
            "q : torch.Size([128, 4, 31, 16])\n",
            "query : torch.Size([128, 33, 64])\n",
            "q : torch.Size([128, 4, 33, 16])\n",
            "query : torch.Size([128, 33, 64])\n",
            "q : torch.Size([128, 4, 33, 16])\n",
            "query : torch.Size([128, 33, 64])\n",
            "q : torch.Size([128, 4, 33, 16])\n",
            "query : torch.Size([128, 33, 64])\n",
            "q : torch.Size([128, 4, 33, 16])\n",
            "query : torch.Size([128, 33, 64])\n",
            "q : torch.Size([128, 4, 33, 16])\n",
            "query : torch.Size([128, 33, 64])\n",
            "q : torch.Size([128, 4, 33, 16])\n",
            "\tTrain Loss: 0.042 | Train PPL:   1.043\n",
            "\t Val. Loss: 9.503 |  Val. PPL: 13404.755\n",
            "query : torch.Size([128, 35, 64])\n",
            "q : torch.Size([128, 4, 35, 16])\n",
            "query : torch.Size([128, 35, 64])\n",
            "q : torch.Size([128, 4, 35, 16])\n",
            "query : torch.Size([128, 35, 64])\n",
            "q : torch.Size([128, 4, 35, 16])\n",
            "query : torch.Size([128, 30, 64])\n",
            "q : torch.Size([128, 4, 30, 16])\n",
            "query : torch.Size([128, 30, 64])\n",
            "q : torch.Size([128, 4, 30, 16])\n",
            "query : torch.Size([128, 30, 64])\n",
            "q : torch.Size([128, 4, 30, 16])\n",
            "query : torch.Size([128, 30, 64])\n",
            "q : torch.Size([128, 4, 30, 16])\n",
            "query : torch.Size([128, 30, 64])\n",
            "q : torch.Size([128, 4, 30, 16])\n",
            "query : torch.Size([128, 30, 64])\n",
            "q : torch.Size([128, 4, 30, 16])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-ab4ef3947f50>\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m# Evaluation on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# test_loss = evaluate(model, test_iterator, criterion)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_loss' is not defined"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          iterator: DataLoader,\n",
        "          optimizer: optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          clip: float):\n",
        "\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in iterator:\n",
        "        src = batch['SRC'].to(device)\n",
        "        trg = batch['TRG'].to(device)\n",
        "\n",
        "        # Assuming src and trg are already tensorized and padded\n",
        "        # If not, you should perform those steps here\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output, enc_attention_scores, _ = model(src, trg)\n",
        "\n",
        "        # Flatten the output and target tensors to compute the loss\n",
        "        output = output[:,:-1,:].reshape(-1, output.shape[-1])\n",
        "        trg = trg[:,1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        break\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module,\n",
        "             iterator: DataLoader,\n",
        "             criterion: nn.Module):\n",
        "\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            src = batch['SRC'].to(device)\n",
        "            trg = batch['TRG'].to(device)\n",
        "\n",
        "            # Assuming src and trg are already tensorized and padded\n",
        "            # If not, you should perform those steps here\n",
        "\n",
        "            output, attention_score, _ = model(src, trg)\n",
        "\n",
        "            # Flatten the output and target tensors to compute the loss\n",
        "            output = output[:,:-1,:].reshape(-1, output.shape[-1])\n",
        "            trg = trg[:,1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "# Training loop\n",
        "for epoch in tqdm(range(N_EPOCHS), total=N_EPOCHS):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    # valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "    else: # early stopping condition\n",
        "        break\n",
        "\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "# Evaluation on test set\n",
        "# test_loss = evaluate(model, test_iterator, criterion)\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxyJad1WEbN8"
      },
      "source": [
        "# IV. Visualization\n",
        "<a id='4'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_Eop7pGEbN8"
      },
      "source": [
        "## 1. Positional embedding visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJKGr5JfEbN8",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(15, 9))\n",
        "cax = ax.matshow(model.encoder.embed_positions.weight.data.cpu().numpy(), aspect='auto',cmap=plt.cm.YlOrRd)\n",
        "fig.colorbar(cax)\n",
        "ax.set_title('Positional Embedding Matrix', fontsize=18)\n",
        "ax.set_xlabel('Embedding Dimension', fontsize=14)\n",
        "ax.set_ylabel('Sequence Length', fontsize=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrLCVOWlEbN8"
      },
      "source": [
        "## 2. Attention visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azwmQfF-EbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from attentionviz import head_view\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLCz7R73EbN8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if not 'attentionviz' in sys.path:\n",
        "  sys.path += ['attentionviz']\n",
        "!pip install regex\n",
        "\n",
        "def call_html():\n",
        "  import IPython\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n",
        "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkV8XEM2EbN9",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "SAMPLE_IDX = 131\n",
        "\n",
        "sample = test_data[SAMPLE_IDX]\n",
        "\n",
        "src_numericalized = torch.LongTensor([SRC_vocab.numericalize(sample['SRC'])]).to(device)\n",
        "trg_numericalized = torch.LongTensor([TRG_vocab.numericalize(sample['TRG'])]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output, enc_attention_score, dec_attention_score = model(src_numericalized, trg_numericalized) # turn off teacher forcing\n",
        "    attention_score = {'self': enc_attention_score, 'cross': dec_attention_score}\n",
        "\n",
        "src_tok = [SRC_vocab.itos[x] for x in src_numericalized.squeeze()]\n",
        "trg_tok = [TRG_vocab.itos[x] for x in trg_numericalized.squeeze()]\n",
        "\n",
        "call_html()\n",
        "head_view(attention_score, src_tok, trg_tok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ejrRprgIDeM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}